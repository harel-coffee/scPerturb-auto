"""
Author: Stefan Peidli
Aim: Snakemake workflow for scperturb analysis.
Date: 15.12.2022
Run: snakemake
Run (dev): snakemake --profile=cubi-dev --jobs 100 -k --use-conda --restart-times 0 --rerun-triggers mtime
DAG: snakemake --forceall --dag | dot -Tpdf > snake_dag.pdf
Rulegraph: snakemake --forceall --rulegraph | dot -Tpdf > snake_rulegraph.pdf
"""

import pandas as pd
import numpy as np
import sys
import yaml
from pathlib import Path

configfile: "../../config.yaml"

TEMPDIR = Path(config["TEMPDIR"])
DOWNDIR = Path(config["DOWNDIR"])

# load list of RNA datasets
df = pd.read_excel('../../metadata/scperturb_dataset_info.xlsx', index_col=[0,1])
sdf = df[df['Modality = Data type'].isin(['RNA', 'RNA + protein (RNA)'])]
datasets = [a+'_'+b for (a,b), row in sdf.iterrows()]
for dataset in ['GasperiniShendure2019_highMOI', 'GasperiniShendure2019_atscale', 'SchraivogelSteinmetz2020_TAP_SCREEN__chromosome_8_screen', 'GehringPachter2019']:
	if dataset in datasets:
		datasets.remove(dataset)
print(datasets)

rule all:
	input:
		expand(TEMPDIR / 'tmp_data_{dataset}.h5', dataset=datasets),
		# expand(TEMPDIR / 'tmp_data_{dataset}.h5', dataset=repoweiss),
		expand('tables/{mode}_{dataset}_tables.csv', dataset=datasets,
			   mode=['etest', 'edist',
			   'earth_mover_distances', 'pseudobulk_correlations',  # TODO: PSEUDOBULK PCA distances
			   # 'simple_confusion', 'simple_test_confusion', 'looc_confusion', 'graph_entropy'
			   ]),
		expand('figures/{mode}_{dataset}.pdf', dataset=datasets,
			   mode=['umap', 'pseudobulk_umap'
			   ])

rule zenodo_download:
	"""
	Downloads all RNA datasets of scperturb from zenodo via wget.
  	"""
	output: DOWNDIR / '{dataset}.h5ad'
	resources:
		partition='short',
		time='4:00:00',
		mem_mb=4000,
		disk_mb=4000
	params:
		sleeptime=np.random.uniform(1,20),
		zenodo_url=config["ZENODO_RNA_URL"]
	shell:
		'''
		sleep {params.sleeptime}
		wget {params.zenodo_url}/files/{dataset}.h5ad -O {output}
		'''

rule prepare_data:
	"""
	Prepares scperturb RNA datasets:
	- Filtering (Cells: min_counts=1000, Genes: min_cells=50)
	- Normalization
	- Log1p trafo
	- Removes perturbations with less than 50 cells, then
	- Subsamples each perturbation to the smallest 
	  number of cells such that each perturbation has at least that many cells
	- Precomputes 2000 HVGs, PCA and KNN graph
	- Adds a convenience annotation, randomly splitting each perturbation in two groups artificially
	  E.g. 50 EGFRi cells --> 25 EGFRi cells + 25 EGFRi_X cells.
  	"""
	input: DOWNDIR / '{dataset}.h5ad'
	output: TEMPDIR / 'tmp_data_{dataset}.h5'
	resources:
		partition='medium',
		time='0-18:00:00',
		mem_mb=64000,
		disk_mb=64000
	run:
		import scanpy as sc
		# project utils
		sys.path.insert(1, '../../')
		from utils import equal_subsampling, random_split

		adata = sc.read(input[0])
		adata.layers['counts'] = adata.X.copy()

		# basic qc and pp
		sc.pp.filter_cells(adata, min_counts=1000)
		sc.pp.normalize_per_cell(adata)
		sc.pp.filter_genes(adata, min_cells=50)
		sc.pp.log1p(adata)

		# subsample against high class imbalance
		adata = equal_subsampling(adata, 'perturbation', N_min=50)
		sc.pp.filter_genes(adata, min_cells=3)  # sanity cleaning

		# select HVGs
		n_var_max = 2000  # max total features to select
		sc.pp.highly_variable_genes(adata, n_top_genes=n_var_max, subset=False, flavor='seurat_v3', layer='counts')
		sc.pp.pca(adata, use_highly_variable=True)
		sc.pp.neighbors(adata)

		# annotate split for evaluation
		random_split(adata)

		adata.write(output[0])

rule e_testing:
	"""
	Computes and plots E-test between each perturbation and 'control' cells.
  	"""
	input: TEMPDIR / 'tmp_data_{dataset}.h5'
	output: 'figures/etest_{dataset}.pdf', 'tables/etest_{dataset}_tables.csv'
	params:
		iterations=1000
	resources:
		partition='medium',
		time='1-23:00:00',
		mem_mb=168000,
		disk_mb=168000
	run:
		import scanpy as sc
		import matplotlib.pyplot as pl
		import seaborn as sns
		# scperturb utils
		sys.path.insert(1, '../../package/src/')
		from scperturb import etest

		adata = sc.read(input[0])
		df = etest(adata, runs=params.iterations)
		df['log10_edist'] = np.log10(np.clip(df.edist, 0, np.infty))

		# export table
		df.to_csv(output[1])

		# plot result
		with sns.axes_style('whitegrid'):
			fig, ax = pl.subplots(1,1, figsize=[6,4], dpi=100)
		sns.scatterplot(data=df[df.index!='control'], x='log10_edist', y='pvalue_adj', hue='significant_adj', palette={True: 'tab:green', False: 'tab:red'})
		sig = np.sum(df['significant_adj'])
		total = len(df)-1  # (removes control)
		ax.set_xticks([0,1,2])
		# ax.set_xticklabels([r'$10^0$', r'$10^1$', r'$10^2$'])
		ax.set_xticklabels([1, 10, 100])
		ax.set_title(f'E-tests for {dataset}\n{sig}/{total} are significant (pv<0.05)')
		ax.set_ylabel('Adjusted p-value')
		ax.set_xlabel('E-distance to unperturbed')
		ax.legend(title='Significant')
		ax.axhline(0.05, c='r', linestyle='--')
		small = df[(df['significant_adj']) & (df.index!='control')].sort_values('edist').iloc[0]
		big = df[(~df['significant_adj']) & (df.index!='control')].sort_values('edist').iloc[-1]
		ax.annotate(f'E-distance\n{np.round(small.edist,2)}', xy=(small.log10_edist, small.pvalue_adj),  xycoords='data',
		            xytext=(0.5, small.pvalue_adj), textcoords='data', fontsize=10,
		            arrowprops=dict(facecolor='black', shrink=0.05),
		            horizontalalignment='right', verticalalignment='center',
		            )
		ax.annotate(f'E-distance\n{np.round(big.edist,2)}', xy=(big.log10_edist, big.pvalue_adj),  xycoords='data',
		            xytext=(2, big.pvalue_adj+0.2), textcoords='data', fontsize=10,
		            arrowprops=dict(facecolor='black', shrink=0.05),
		            horizontalalignment='right', verticalalignment='center',
		            )
		pl.savefig(output[0], bbox_inches='tight', dpi=120)
		pl.close()

rule move_some_earth:
	"""
	Computes the 2-Wasserstein distance between all perturbation pairs.
  	"""
	input: TEMPDIR / 'tmp_data_{dataset}.h5'
	output: 'tables/earth_mover_distances_{dataset}_tables.csv'
	resources:
		partition='short',
		time='4:00:00',
		mem_mb=168000,
		disk_mb=168000
	run:
		import scanpy as sc
		import matplotlib.pyplot as pl
		import seaborn as sns
		import ot
		from tqdm import tqdm
		from sklearn.metrics import pairwise_distances
		def transport (source, target):
			n = source.shape[-1]
			m = target.shape[-1]
			a, b = np.ones((n,)) / n, np.ones((m,)) / m  # uniform distribution on samples

			# loss matrix
			loss_matrix = pairwise_distances(source.T, target.T, metric='sqeuclidean')
			loss_matrix /= loss_matrix.max()

			transport_matrix, log = ot.emd(a, b, loss_matrix, log=True)
			cost = log['cost']
			return cost
		adata = sc.read(input[0])
		groups = pd.unique(adata.obs['perturbation'])
		df = pd.DataFrame(index=groups, columns=groups, dtype=float)
		for i, p1 in enumerate(tqdm(groups)):
			for p2 in groups[i:]:
				x1 = adata[adata.obs.perturbation==p1].obsm['X_pca'].copy()
				x2 = adata[adata.obs.perturbation==p2].obsm['X_pca'].copy()
				cost = transport(x1, x2)
				df.loc[p1, p2] = cost
				df.loc[p2, p1] = cost
		df.to_csv(output[0])

rule e_distance:
	"""
	Computes and plots the E-distance between all perturbation pairs.
  	"""
	input: TEMPDIR / 'tmp_data_{dataset}.h5'
	output: 'figures/edist_{dataset}.pdf', 'tables/edist_{dataset}_tables.csv'
	resources:
		partition='short',
		time='4:00:00',
		mem_mb=168000,
		disk_mb=168000
	run:
		import scanpy as sc
		import matplotlib.pyplot as pl
		# scperturb utils
		sys.path.insert(1, '../../package/src/')
		from scperturb import edist
		# project utils
		sys.path.insert(1, '../../')
		from utils import cluster_matrix

		adata = sc.read(input[0])
		tab = edist(adata, 'perturbation', obsm_key='X_pca')
		tab.to_csv(output[1])

		# plot
		tab = (1/tab).replace([np.inf, -np.inf], 0, inplace=False)
		tab = cluster_matrix(tab, 'both')
		plot_heatmap(tab, wildcards.dataset+' edistances')
		pl.savefig(output[0], bbox_inches='tight', dpi=120)
		pl.close()

rule umap:
	"""
	Plots a umap of cells with perturbations as colors.
  	"""
	input: TEMPDIR / 'tmp_data_{dataset}.h5'
	output: 'figures/umap_{dataset}.pdf'
	resources:
		partition='short',
		time='4:00:00',
		mem_mb=168000,
		disk_mb=168000
	run:
		import scanpy as sc
		import scvelo as scv
		import matplotlib.pyplot as pl
		adata = sc.read(input[0])
		sc.tl.umap(adata)
		scv.pl.scatter(adata, color='perturbation', show=False, dpi=120, legend_loc='right margin')
		pl.savefig(output[0], bbox_inches='tight', dpi=120)
		pl.close()

rule pseudobulk_umap:
	"""
	Plots a umap where each point is the pseudobulk of one perturbation.
  	"""
	input: TEMPDIR / 'tmp_data_{dataset}.h5'
	output: 'figures/pseudobulk_umap_{dataset}.pdf'
	resources:
		partition='short',
		time='4:00:00',
		mem_mb=168000,
		disk_mb=168000
	run:
		import scanpy as sc
		import scvelo as scv
		import matplotlib.pyplot as pl
		# project utils
		sys.path.insert(1, '../../')
		from utils import pseudo_bulk

		adata = sc.read(input[0])
		bdata = pseudo_bulk(adata, ['perturbation'])
		sc.pp.normalize_per_cell(bdata)
		sc.pp.log1p(bdata)
		sc.pp.pca(bdata)
		sc.pp.neighbors(bdata)
		sc.tl.umap(bdata)
		bdata.obs['perturbation'] = bdata.obs['perturbation'].astype('category')
		scv.pl.scatter(bdata, color='perturbation', show=False, dpi=120, legend_loc='right margin')
		pl.savefig(output[0], bbox_inches='tight', dpi=120)
		pl.close()

rule pseudobulk_correlations:
	"""
	Computes the pairwise correlation between pseudobulked perturbations in PCA space.
  	"""
	input: TEMPDIR / 'tmp_data_{dataset}.h5'
	output: 'figures/pseudobulk_correlations_{dataset}.pdf', 'tables/pseudobulk_correlations_{dataset}_tables.csv'
	resources:
		partition='short',
		time='4:00:00',
		mem_mb=168000,
		disk_mb=168000
	run:
		import scanpy as sc
		import matplotlib.pyplot as pl
		# project utils
		sys.path.insert(1, '../../')
		from utils import pseudo_bulk, cluster_matrix

		adata = sc.read(input[0])
		bdata = pseudo_bulk(adata, ['perturbation'])
		sc.pp.normalize_per_cell(bdata)
		sc.pp.log1p(bdata)
		sc.pp.pca(bdata)
		Z = bdata.obsm['X_pca']
		df = pd.DataFrame(Z, index=bdata.obs.perturbation)
		pca_corr = df.T.corr()
		tab = cluster_matrix(pca_corr, 'both')
		# plot
		plot_heatmap(tab, wildcards.dataset+' pseudobulk correlations')
		# export
		pl.savefig(output[0], bbox_inches='tight', dpi=120)
		tab.to_csv(output[1])
		pl.close()

# rule simple_confusion:
# 	input: TEMPDIR / 'tmp_data_{dataset}.h5'
# 	output: 'figures/simple_confusion_{dataset}.pdf', 'tables/simple_confusion_{dataset}_tables.csv'
# 	resources:
# 		partition='short',
# 		time='4:00:00',
# 		mem_mb=mem*1000,
# 		disk_mb=mem*1000
# 	run:
# 		adata = sc.read(input[0])
# 		conf_mat, classes = simple_classifier_confusion(adata, 'X_pca', 'perturbation', test_size_fraction=0.2, n_nodes=0, method='SVC',
# 														symmetrize=False, col_normalize=False, cluster=False)
# 		pl.figure(figsize=[10,8])
# 		tab = pd.DataFrame(zscore(conf_mat.values, axis=0), classes, classes)
# 		tab = 0.5 * (tab+tab.T)
# 		tab = cluster_matrix(tab, 'both')
# 		raw_tab = pd.DataFrame(conf_mat, index=classes, columns=classes)
# 		# plot
# 		plot_heatmap(tab, wildcards.dataset+' simple confusion')
# 		# export
# 		pl.savefig(output[0], bbox_inches='tight', dpi=120)
# 		raw_tab.to_csv(output[1])
# 		pl.close()

# rule simple_test_confusion:
# 	input: TEMPDIR / 'tmp_data_{dataset}.h5'
# 	output: 'figures/simple_test_confusion_{dataset}.pdf', 'tables/simple_test_confusion_{dataset}_tables.csv'
# 	resources:
# 		partition='short',
# 		time='4:00:00',
# 		mem_mb=mem*1000,
# 		disk_mb=mem*1000
# 	run:
# 		adata = sc.read(input[0])
# 		conf_mat, classes = simple_classifier_confusion(adata, 'X_pca', 'perturbation', test_size_fraction=0.2, n_nodes=0, method='SVC',
# 														symmetrize=False, col_normalize=False, cluster=False, propagate='test')
# 		pl.figure(figsize=[10,8])
# 		tab = pd.DataFrame(zscore(conf_mat.values, axis=0), classes, classes)
# 		tab = 0.5 * (tab+tab.T)
# 		tab = cluster_matrix(tab, 'both')
# 		raw_tab = pd.DataFrame(conf_mat, index=classes, columns=classes)
# 		# plot
# 		plot_heatmap(tab, wildcards.dataset+' simple confusion')
# 		# export
# 		pl.savefig(output[0], bbox_inches='tight', dpi=120)
# 		raw_tab.to_csv(output[1])
# 		pl.close()

# rule looc_confusion:
# 	input: TEMPDIR / 'tmp_data_{dataset}.h5'
# 	output: 'figures/looc_confusion_{dataset}.pdf', 'tables/looc_confusion_{dataset}_tables.csv'
# 	resources:
# 		partition='short',
# 		time='4:00:00',
# 		mem_mb=mem*1000,
# 		disk_mb=mem*1000
# 	run:
# 		adata = sc.read(input[0])
# 		tab = leave_one_out_classification(adata, 'X_pca', 'perturbation', method='SVC', plot=True, show=False, plot_each=False)
# 		# plot
# 		plot_heatmap(tab, wildcards.dataset+' looc')
# 		# export
# 		pl.savefig(output[0], bbox_inches='tight', dpi=120)
# 		tab.to_csv(output[1])
# 		pl.close()

# rule graph_entropy:
# 	input: TEMPDIR / 'tmp_data_{dataset}.h5'
# 	output: 'figures/graph_entropy_{dataset}.pdf', 'tables/graph_entropy_{dataset}_tables.csv'
# 	resources:
# 		partition='short',
# 		time='4:00:00',
# 		mem_mb=mem*1000,
# 		disk_mb=mem*1000
# 	run:
# 		adata = sc.read(input[0])
# 		tab = simil(adata, groupby='perturbation', plot=False)
# 		# plot
# 		plot_heatmap(tab, wildcards.dataset+' Graph label entropy')
# 		# export
# 		pl.savefig(output[0], bbox_inches='tight', dpi=120)
# 		tab.to_csv(output[1])
# 		pl.close()
