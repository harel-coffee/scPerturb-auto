# snakemake --profile=cubi-v1 --jobs 100 -k --use-conda --restart-times 0
# snakemake --forceall --dag | dot -Tpdf > snake_dag.pdf
# snakemake --forceall --rulegraph | dot -Tpdf > snake_rulegraph.pdf

mem = 168# 2000
partition = 'short'#''highmem

import pandas as pd
import numpy as np
import os
import sys
import matplotlib.pyplot as pl

SDIR = "/fast/scratch/users/peidlis_c/perturbation_resource_paper/"
WDIR = '/fast/work/users/peidlis_c/data/perturbation_resource_paper/'  # data directory on work
CDIR = '/fast/work/users/peidlis_c/projects/perturbation_resource_paper/single_cell_perturbation_data/code/'  # code directory on work
UTILS = "/fast/work/users/peidlis_c/utils/single_cell_rna_seq/scrnaseq_utils/"  # my utils
sys.path.insert(1, UTILS)
from scrnaseq_util_functions import *
sys.path.insert(1, CDIR)
from utils import *

# dict mapping dataset names to their paths
h5_files = {}
for path, subdirs, files in os.walk(WDIR):
	for name in files:
		if '.h5ad' in name:
			h5_files[name[:-5]] = os.path.join(path, name)
for k in ['exampledataset', 'NormanWeissman2019_raw']:
	if k in h5_files.keys(): del h5_files[k]
for k in ['PapalexiSatija2021_eccite_arrayed_protein', 'PapalexiSatija2021_eccite_protein', 'FrangiehIzar2021_protein']:
	if k in h5_files.keys(): del h5_files[k]
for k in ['gene_scores', 'ChromVar', 'LSI_embedding', 'markerpeak_target', 'peak_bc']:
	if k in h5_files.keys(): del h5_files[k]
for k in ['GasperiniShendure2019_highMOI', 'GasperiniShendure2019_atscale', 'SchraivogelSteinmetz2020_TAP_SCREEN__chromosome_8_screen', 'GehringPachter2019']:
	if k in h5_files.keys(): del h5_files[k]
repoweiss_h5_files = {}
f = list(h5_files.keys())
for k in f:
	if 'Replogle' in k:
		repoweiss_h5_files[k] = h5_files[k]
		del h5_files[k]

print(list(h5_files.keys()))

def plot_heatmap(tab, title):
	fig, ax = pl.subplots(figsize=[10,8], dpi=120)
	sns.heatmap(tab, robust=True, ax=ax)
	ax.set_xticks(np.arange(len(tab))+.5)
	ax.set_xticklabels(tab.index, fontsize=6)
	ax.set_yticks(np.arange(len(tab))+.5)
	ax.set_yticklabels(tab.index, fontsize=6)
	ax.set_title(title)

rule all:
	input:
		# expand(SDIR+'tmp_data_{dataset}.h5', dataset=repoweiss_h5_files.keys()),
		expand('tables/{mode}_{dataset}_tables.csv', dataset=list(h5_files.keys())+list(repoweiss_h5_files.keys()),
			   mode=[
		# 	   # 'etest',
		# 	   #'earth_mover_distances',
			   'pairwise_pca_distances',
		# 	   # 'pairwise_mean_pca_distances', ,
		# 	   # 'simple_confusion', 'simple_test_confusion', 'looc_confusion',
		# 	   # 'pseudobulk_correlations', 'graph_entropy'
			   ]),
		# expand('figures/{mode}_{dataset}.pdf', dataset=h5_files.keys(),
		# 	   mode=['umap', 'pseudobulk_umap'
		# 	   ])

# rule prep_data:
# 	output:
# 		SDIR+'tmp_data_{dataset}.h5'
# 	resources:
# 		partition='medium',
# 		mem='64g',##str(mem)+'g',
# 		time='2-00:00:00',
# 		mem_mb=64000,#mem*1000,
# 		disk_mb=64000#mem*1000
# 	run:
# 		path = h5_files[wildcards.dataset]
# 		adata = sc.read(path)
# 		adata.layers['counts'] = adata.X.copy()
#
# 		# basic qc and pp
# 		sc.pp.filter_cells(adata, min_counts=1000)
# 		sc.pp.normalize_per_cell(adata)
# 		sc.pp.filter_genes(adata, min_cells=50)
# 		sc.pp.log1p(adata)
#
# 		# high class imbalance
# 		adata = equal_subsampling(adata, 'perturbation', N_min=50)
# 		sc.pp.filter_genes(adata, min_cells=3)  # sanity cleaning
#
# 		# select HVGs
# 		n_var_max = 2000  # max total features to select
# 		sc.pp.highly_variable_genes(adata, n_top_genes=n_var_max, subset=False, flavor='seurat_v3', layer='counts')
# 		sc.pp.pca(adata, use_highly_variable=True)
# 		sc.pp.neighbors(adata)
# 		adata.write(output[0])

rule prep_data_repoweiss:
	output:
		SDIR+'tmp_data_{dataset}.h5'
	resources:
		partition='highmem',
		mem='500g',##str(mem)+'g',
		time='2-00:00:00',
		mem_mb=500000,#mem*1000,
		disk_mb=500000#mem*1000
	run:
		path = repoweiss_h5_files[wildcards.dataset]
		adata = sc.read(path)
		adata.layers['counts'] = adata.X.copy()

		# basic qc and pp
		sc.pp.filter_cells(adata, min_counts=1000)
		sc.pp.normalize_per_cell(adata)
		sc.pp.filter_genes(adata, min_cells=50)
		sc.pp.log1p(adata)

		# high class imbalance
		adata = equal_subsampling(adata, 'perturbation', N_min=50)
		sc.pp.filter_genes(adata, min_cells=3)  # sanity cleaning

		# select HVGs
		n_var_max = 2000  # max total features to select
		sc.pp.highly_variable_genes(adata, n_top_genes=n_var_max, subset=False, flavor='seurat_v3', layer='counts')
		sc.pp.pca(adata, use_highly_variable=True)
		sc.pp.neighbors(adata)
		adata.write(output[0])

rule e_testing:
	input: SDIR+'tmp_data_{dataset}.h5'
	output: 'figures/etest_{dataset}.pdf', 'tables/etest_{dataset}_tables.csv'
	resources:
		partition='medium',
		mem=str(mem)+'g',
		time='1-23:00:00',
		mem_mb=mem*1000,
		disk_mb=mem*1000
	run:
		adata = sc.read(input[0])
		tab = etest(adata, runs=100)

		# plot result
		pseudocount = 1
		tab['significant'] = tab.pvalue < 0.05
		with sns.axes_style('whitegrid'):
		    sns.scatterplot(data=tab, x=tab.edist+pseudocount, y=tab.pvalue, hue='significant')
		pl.xlabel(f'Log {pseudocount}+E-distance to unperturbed')
		sig = np.sum(tab['significant'])
		total = len(tab)-1
		pl.title(f'E-test results for {wildcards.dataset}\n{sig}/{total} are significant (pv<0.05)')
		pl.xscale('log')
		pl.ylabel('p-value')
		pl.axhline(0.05, c='r', linestyle='--')
		pl.savefig(output[0], bbox_inches='tight', dpi=120)
		pl.close()

		# export table
		tab.to_csv(output[1])

rule move_some_earth:
	input: SDIR+'tmp_data_{dataset}.h5'
	output: 'tables/earth_mover_distances_{dataset}_tables.csv'
	resources:
		partition=partition,
		mem=str(mem)+'g',
		time='4:00:00',
		mem_mb=mem*1000,
		disk_mb=mem*1000
	run:
		import ot
		from sklearn.metrics import pairwise_distances
		def transport (source, target):
			n = source.shape[-1]
			m = target.shape[-1]
			a, b = np.ones((n,)) / n, np.ones((m,)) / m  # uniform distribution on samples

			# loss matrix
			loss_matrix = pairwise_distances(source.T, target.T, metric='sqeuclidean')
			loss_matrix /= loss_matrix.max()

			transport_matrix, log = ot.emd(a, b, loss_matrix, log=True)
			cost = log['cost']
			return cost
		adata = sc.read(input[0])
		groups = pd.unique(adata.obs['perturbation'])
		df = pd.DataFrame(index=groups, columns=groups, dtype=float)
		for i, p1 in enumerate(tqdm(groups)):
			for p2 in groups[i:]:
				x1 = adata[adata.obs.perturbation==p1].obsm['X_pca'].copy()
				x2 = adata[adata.obs.perturbation==p2].obsm['X_pca'].copy()
				cost = transport(x1, x2)
				df.loc[p1, p2] = cost
				df.loc[p2, p1] = cost
		df.to_csv(output[0])

rule pairwise_mean_pca_distances:
	input: SDIR+'tmp_data_{dataset}.h5'
	output: 'figures/pairwise_mean_pca_distances_{dataset}.pdf', 'tables/pairwise_mean_pca_distances_{dataset}_tables.csv'
	resources:
		partition=partition,
		mem=str(mem)+'g',
		time='4:00:00',
		mem_mb=mem*1000,
		disk_mb=mem*1000
	run:
		adata = sc.read(input[0])
		tab = pairwise_mean_pca_distances(adata, 'perturbation', obsm_key='X_pca')
		tab.to_csv(output[1])
		# plot
		tab = (1/tab).replace([np.inf, -np.inf], 0, inplace=False)
		tab = cluster_matrix(tab, 'both')
		plot_heatmap(tab, wildcards.dataset+' pseudobulk pca distances')
		pl.savefig(output[0], bbox_inches='tight', dpi=120)
		pl.close()

rule pairwise_pca_distances:
	input: SDIR+'tmp_data_{dataset}.h5'
	output: 'figures/pairwise_pca_distances_{dataset}.pdf', 'tables/pairwise_pca_distances_{dataset}_tables.csv'
	resources:
		partition=partition,
		mem=str(mem)+'g',
		time='4:00:00',
		mem_mb=mem*1000,
		disk_mb=mem*1000
	run:
		adata = sc.read(input[0])
		tab = pairwise_pca_distances(adata, 'perturbation', obsm_key='X_pca')
		tab.to_csv(output[1])
		# plot
		tab = cluster_matrix(1/tab, 'both')
		plot_heatmap(tab, wildcards.dataset+' mean pairwise pca distance')
		pl.savefig(output[0], bbox_inches='tight', dpi=120)
		pl.close()

rule umap:
	input: SDIR+'tmp_data_{dataset}.h5'
	output: 'figures/umap_{dataset}.pdf'
	resources:
		partition=partition,
		mem=str(mem)+'g',
		time='4:00:00',
		mem_mb=mem*1000,
		disk_mb=mem*1000
	run:
		adata = sc.read(input[0])
		sc.tl.umap(adata)
		scv.pl.scatter(adata, color='perturbation', show=False, dpi=120, legend_loc='right margin')
		pl.savefig(output[0], bbox_inches='tight', dpi=120)
		pl.close()

rule pseudobulk_umap:
	input: SDIR+'tmp_data_{dataset}.h5'
	output: 'figures/pseudobulk_umap_{dataset}.pdf'
	resources:
		partition=partition,
		mem=str(mem)+'g',
		time='4:00:00',
		mem_mb=mem*1000,
		disk_mb=mem*1000
	run:
		adata = sc.read(input[0])
		bdata = pseudo_bulk(adata, ['perturbation'])
		sc.pp.normalize_per_cell(bdata)
		sc.pp.log1p(bdata)
		sc.pp.pca(bdata)
		sc.pp.neighbors(bdata)
		sc.tl.umap(bdata)
		bdata.obs['perturbation'] = bdata.obs['perturbation'].astype('category')
		scv.pl.scatter(bdata, color='perturbation', show=False, dpi=120, legend_loc='right margin')
		pl.savefig(output[0], bbox_inches='tight', dpi=120)
		pl.close()

rule pseudobulk_correlations:
	input: SDIR+'tmp_data_{dataset}.h5'
	output: 'figures/pseudobulk_correlations_{dataset}.pdf', 'tables/pseudobulk_correlations_{dataset}_tables.csv'
	resources:
		partition=partition,
		mem=str(mem)+'g',
		time='4:00:00',
		mem_mb=mem*1000,
		disk_mb=mem*1000
	run:
		adata = sc.read(input[0])
		bdata = pseudo_bulk(adata, ['perturbation'])
		sc.pp.normalize_per_cell(bdata)
		sc.pp.log1p(bdata)
		sc.pp.pca(bdata)
		Z = bdata.obsm['X_pca']
		df = pd.DataFrame(Z, index=bdata.obs.perturbation)
		pca_corr = df.T.corr()
		tab = cluster_matrix(pca_corr, 'both')
		# plot
		plot_heatmap(tab, wildcards.dataset+' pseudobulk correlations')
		# export
		pl.savefig(output[0], bbox_inches='tight', dpi=120)
		tab.to_csv(output[1])
		pl.close()

rule simple_confusion:
	input: SDIR+'tmp_data_{dataset}.h5'
	output: 'figures/simple_confusion_{dataset}.pdf', 'tables/simple_confusion_{dataset}_tables.csv'
	resources:
		partition=partition,
		mem=str(mem)+'g',
		time='4:00:00',
		mem_mb=mem*1000,
		disk_mb=mem*1000
	run:
		adata = sc.read(input[0])
		conf_mat, classes = simple_classifier_confusion(adata, 'X_pca', 'perturbation', test_size_fraction=0.2, n_nodes=0, method='SVC',
														symmetrize=False, col_normalize=False, cluster=False)
		pl.figure(figsize=[10,8])
		tab = pd.DataFrame(zscore(conf_mat.values, axis=0), classes, classes)
		tab = 0.5 * (tab+tab.T)
		tab = cluster_matrix(tab, 'both')
		raw_tab = pd.DataFrame(conf_mat, index=classes, columns=classes)
		# plot
		plot_heatmap(tab, wildcards.dataset+' simple confusion')
		# export
		pl.savefig(output[0], bbox_inches='tight', dpi=120)
		raw_tab.to_csv(output[1])
		pl.close()

rule simple_test_confusion:
	input: SDIR+'tmp_data_{dataset}.h5'
	output: 'figures/simple_test_confusion_{dataset}.pdf', 'tables/simple_test_confusion_{dataset}_tables.csv'
	resources:
		partition=partition,
		mem=str(mem)+'g',
		time='4:00:00',
		mem_mb=mem*1000,
		disk_mb=mem*1000
	run:
		adata = sc.read(input[0])
		conf_mat, classes = simple_classifier_confusion(adata, 'X_pca', 'perturbation', test_size_fraction=0.2, n_nodes=0, method='SVC',
														symmetrize=False, col_normalize=False, cluster=False, propagate='test')
		pl.figure(figsize=[10,8])
		tab = pd.DataFrame(zscore(conf_mat.values, axis=0), classes, classes)
		tab = 0.5 * (tab+tab.T)
		tab = cluster_matrix(tab, 'both')
		raw_tab = pd.DataFrame(conf_mat, index=classes, columns=classes)
		# plot
		plot_heatmap(tab, wildcards.dataset+' simple confusion')
		# export
		pl.savefig(output[0], bbox_inches='tight', dpi=120)
		raw_tab.to_csv(output[1])
		pl.close()

rule looc_confusion:
	input: SDIR+'tmp_data_{dataset}.h5'
	output: 'figures/looc_confusion_{dataset}.pdf', 'tables/looc_confusion_{dataset}_tables.csv'
	resources:
		partition=partition,
		mem=str(mem)+'g',
		time='4:00:00',
		mem_mb=mem*1000,
		disk_mb=mem*1000
	run:
		adata = sc.read(input[0])
		tab = leave_one_out_classification(adata, 'X_pca', 'perturbation', method='SVC', plot=True, show=False, plot_each=False)
		# plot
		plot_heatmap(tab, wildcards.dataset+' looc')
		# export
		pl.savefig(output[0], bbox_inches='tight', dpi=120)
		tab.to_csv(output[1])
		pl.close()

rule graph_entropy:
	input: SDIR+'tmp_data_{dataset}.h5'
	output: 'figures/graph_entropy_{dataset}.pdf', 'tables/graph_entropy_{dataset}_tables.csv'
	resources:
		partition=partition,
		mem=str(mem)+'g',
		time='4:00:00',
		mem_mb=mem*1000,
		disk_mb=mem*1000
	run:
		adata = sc.read(input[0])
		tab = simil(adata, groupby='perturbation', plot=False)
		# plot
		plot_heatmap(tab, wildcards.dataset+' Graph label entropy')
		# export
		pl.savefig(output[0], bbox_inches='tight', dpi=120)
		tab.to_csv(output[1])
		pl.close()
