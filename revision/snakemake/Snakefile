"""
Author: Stefan Peidli
Aim: Snakemake workflow for scperturb analysis.
Date: 15.12.2022
Run: snakemake
Run (dev): snakemake --profile=cubi-dev --jobs 100 -k --use-conda --restart-times 0 --rerun-triggers mtime
DAG: snakemake --forceall --dag | dot -Tpdf > snake_dag.pdf
Rulegraph: snakemake --forceall --rulegraph | dot -Tpdf > snake_rulegraph.pdf
"""

import pandas as pd
import numpy as np
import sys
import yaml
from pathlib import Path

# Load configs
configfile: "../../config.yaml"
TEMPDIR = Path(config["TEMPDIR"])
DOWNDIR = Path(config["DOWNDIR"])

# Load list of RNA datasets available via scPerturb
df = pd.read_excel('../../metadata/scperturb_dataset_info.xlsx', index_col=[0,1])  # supplemental table
sdf = df[df['Modality = Data type'].isin(['RNA', 'RNA + protein (RNA)'])]
datasets = [a+'_'+b for (a,b), row in sdf.iterrows()]
for dataset in ['GasperiniShendure2019_highMOI', 'GasperiniShendure2019_atscale', 'SchraivogelSteinmetz2020_TAP_SCREEN__chromosome_8_screen', 'GehringPachter2019']:
	if dataset in datasets:
		datasets.remove(dataset)
print(datasets)

def leftsided_chebyshev_nodes(N):
	# Takes negative chebyshev nodes of first kind, forces 0 to be included, then adds 1 to all.
	return np.polynomial.chebyshev.chebpts1(N*2+1)[:(N+1)] + 1

rule all:
	input:
		# expand(DOWNDIR / '{dataset}.h5ad', dataset=datasets),
		expand(TEMPDIR / 'tmp_data_{dataset}.h5', dataset=datasets),
		expand('tables/{mode}_{dataset}_tables.csv', dataset=datasets,
			   mode=['etest', 'edist',
			   'earth_mover_distances', 'pseudobulk_correlations',  # TODO: PSEUDOBULK PCA distances
			   ]),
		expand('figures/{mode}_{dataset}.pdf', dataset=datasets,
			   mode=['umap', 'pseudobulk_umap'
			   ]),
		expand('tables/{mode}_{dataset}_tables.csv', dataset=['PapalexiSatija2021_eccite_RNA', 'NormanWeissman2019_filtered'],
			   mode=['effect_of_ncounts', 'effect_of_ncells', 'effect_of_nHVGs', 'effect_of_multi_vs_single_PCA'])

rule zenodo_download:
	"""
	Downloads all RNA datasets of scperturb from zenodo via wget.
  	"""
	output: DOWNDIR / '{dataset}.h5ad'
	resources:
		partition='short',
		time='4:00:00',
		mem_mb=4000,
		disk_mb=4000
	params:
		sleeptime=np.random.uniform(1,20),
		zenodo_url=config["ZENODO_RNA_URL"]
	shell:
		'''
		sleep {params.sleeptime}
		wget {params.zenodo_url}/files/{wildcards.dataset}.h5ad -O {output}
		'''

rule prepare_data:
	"""
	Prepares scperturb RNA datasets:
	- Filtering (Cells: min_counts=1000, Genes: min_cells=50)
	- Normalization
	- Log1p trafo
	- Removes perturbations with less than 50 cells, then
	- Subsamples each perturbation to the smallest 
	  number of cells such that each perturbation has at least that many cells
	- Precomputes 2000 HVGs, PCA and KNN graph
	- Adds a convenience annotation, randomly splitting each perturbation in two groups artificially
	  E.g. 50 EGFRi cells --> 25 EGFRi cells + 25 EGFRi_X cells.
  	"""
	input: DOWNDIR / '{dataset}.h5ad'
	output: TEMPDIR / 'tmp_data_{dataset}.h5'
	resources:
		partition='medium',
		time='0-18:00:00',
		mem_mb=64000,
		disk_mb=64000
	run:
		import scanpy as sc
		# project utils
		sys.path.insert(1, '../../')
		from utils import equal_subsampling, random_split

		adata = sc.read(input[0])
		adata.layers['counts'] = adata.X.copy()

		# basic qc and pp
		sc.pp.filter_cells(adata, min_counts=1000)
		sc.pp.normalize_per_cell(adata)
		sc.pp.filter_genes(adata, min_cells=50)
		sc.pp.log1p(adata)
		print('Shape after filtering: ', adata.shape)

		# subsample against high class imbalance
		adata = equal_subsampling(adata, 'perturbation', N_min=50)
		sc.pp.filter_genes(adata, min_cells=3)  # sanity cleaning
		print('Shape after equal-subsampling: ', adata.shape)

		# select HVGs
		n_var_max = 2000  # max total features to select
		sc.pp.highly_variable_genes(adata, n_top_genes=n_var_max, subset=False, flavor='seurat_v3', layer='counts')
		sc.pp.pca(adata, use_highly_variable=True)
		sc.pp.neighbors(adata)

		# annotate split for evaluation
		random_split(adata)

		adata.write(output[0])
		print('Control in perturbation column:', 'control' in adata.obs.perturbation)

rule e_testing:
	"""
	Computes and plots E-test between each perturbation and 'control' cells.
  	"""
	input: TEMPDIR / 'tmp_data_{dataset}.h5'
	output: 'figures/etest_{dataset}.pdf', 'tables/etest_{dataset}_tables.csv'
	params:
		iterations=1000
	resources:
		partition='medium',
		time='1-23:00:00',
		mem_mb=168000,
		disk_mb=168000
	run:
		import scanpy as sc
		import matplotlib.pyplot as pl
		import seaborn as sns
		# scperturb utils
		sys.path.insert(1, '../../package/src/')
		from scperturb import etest

		adata = sc.read(input[0])
		df = etest(adata, runs=params.iterations)
		df['log10_edist'] = np.log10(np.clip(df.edist, 0, np.infty))

		# export table
		df.to_csv(output[1])

		# plot result
		with sns.axes_style('whitegrid'):
			fig, ax = pl.subplots(1,1, figsize=[6,4], dpi=100)
		sns.scatterplot(data=df[df.index!='control'], x='log10_edist', y='pvalue_adj', hue='significant_adj', palette={True: 'tab:green', False: 'tab:red'})
		sig = np.sum(df['significant_adj'])
		total = len(df)-1  # (removes control)
		ax.set_xticks([0,1,2])
		# ax.set_xticklabels([r'$10^0$', r'$10^1$', r'$10^2$'])
		ax.set_xticklabels([1, 10, 100])
		ax.set_title(f'E-tests for {wildcards.dataset}\n{sig}/{total} are significant (pv<0.05)')
		ax.set_ylabel('Adjusted p-value')
		ax.set_xlabel('E-distance to unperturbed')
		ax.legend(title='Significant')
		ax.axhline(0.05, c='r', linestyle='--')
		small = df[(df['significant_adj']) & (df.index!='control')].sort_values('edist').iloc[0]
		big = df[(~df['significant_adj']) & (df.index!='control')].sort_values('edist').iloc[-1]
		ax.annotate(f'E-distance\n{np.round(small.edist,2)}', xy=(small.log10_edist, small.pvalue_adj),  xycoords='data',
		            xytext=(0.5, small.pvalue_adj), textcoords='data', fontsize=10,
		            arrowprops=dict(facecolor='black', shrink=0.05),
		            horizontalalignment='right', verticalalignment='center',
		            )
		ax.annotate(f'E-distance\n{np.round(big.edist,2)}', xy=(big.log10_edist, big.pvalue_adj),  xycoords='data',
		            xytext=(2, big.pvalue_adj+0.2), textcoords='data', fontsize=10,
		            arrowprops=dict(facecolor='black', shrink=0.05),
		            horizontalalignment='right', verticalalignment='center',
		            )
		pl.savefig(output[0], bbox_inches='tight', dpi=120)
		pl.close()

rule move_some_earth:
	"""
	Computes the 2-Wasserstein distance between all perturbation pairs.
  	"""
	input: TEMPDIR / 'tmp_data_{dataset}.h5'
	output: 'tables/earth_mover_distances_{dataset}_tables.csv'
	resources:
		partition='short',
		time='4:00:00',
		mem_mb=168000,
		disk_mb=168000
	run:
		import scanpy as sc
		import matplotlib.pyplot as pl
		import seaborn as sns
		import ot
		from tqdm import tqdm
		from sklearn.metrics import pairwise_distances
		def transport (source, target):
			n = source.shape[-1]
			m = target.shape[-1]
			a, b = np.ones((n,)) / n, np.ones((m,)) / m  # uniform distribution on samples

			# loss matrix
			loss_matrix = pairwise_distances(source.T, target.T, metric='sqeuclidean')
			loss_matrix /= loss_matrix.max()

			transport_matrix, log = ot.emd(a, b, loss_matrix, log=True)
			cost = log['cost']
			return cost
		adata = sc.read(input[0])
		groups = pd.unique(adata.obs['perturbation'])
		df = pd.DataFrame(index=groups, columns=groups, dtype=float)
		for i, p1 in enumerate(tqdm(groups)):
			for p2 in groups[i:]:
				x1 = adata[adata.obs.perturbation==p1].obsm['X_pca'].copy()
				x2 = adata[adata.obs.perturbation==p2].obsm['X_pca'].copy()
				cost = transport(x1, x2)
				df.loc[p1, p2] = cost
				df.loc[p2, p1] = cost
		df.to_csv(output[0])

rule e_distance:
	"""
	Computes and plots the E-distance between all perturbation pairs.
  	"""
	input: TEMPDIR / 'tmp_data_{dataset}.h5'
	output: 'figures/edist_{dataset}.pdf', 'tables/edist_{dataset}_tables.csv'
	resources:
		partition='short',
		time='4:00:00',
		mem_mb=168000,
		disk_mb=168000
	run:
		import scanpy as sc
		import matplotlib.pyplot as pl
		# scperturb utils
		sys.path.insert(1, '../../package/src/')
		from scperturb import edist
		# project utils
		sys.path.insert(1, '../../')
		from utils import cluster_matrix, plot_heatmap

		adata = sc.read(input[0])
		tab = edist(adata, 'perturbation', obsm_key='X_pca')
		tab.to_csv(output[1])

		# plot
		tab = (1/tab).replace([np.inf, -np.inf], 0, inplace=False)
		tab = cluster_matrix(tab, 'both')
		plot_heatmap(tab, wildcards.dataset+' edistances')
		pl.savefig(output[0], bbox_inches='tight', dpi=120)
		pl.close()

rule umap:
	"""
	Plots a umap of cells with perturbations as colors.
  	"""
	input: TEMPDIR / 'tmp_data_{dataset}.h5'
	output: 'figures/umap_{dataset}.pdf'
	resources:
		partition='short',
		time='4:00:00',
		mem_mb=168000,
		disk_mb=168000
	run:
		import scanpy as sc
		import scvelo as scv
		import matplotlib.pyplot as pl
		adata = sc.read(input[0])
		sc.tl.umap(adata)
		scv.pl.scatter(adata, color='perturbation', show=False, dpi=120, legend_loc='right margin')
		pl.savefig(output[0], bbox_inches='tight', dpi=120)
		pl.close()

rule pseudobulk_umap:
	"""
	Plots a umap where each point is the pseudobulk of one perturbation.
  	"""
	input: TEMPDIR / 'tmp_data_{dataset}.h5'
	output: 'figures/pseudobulk_umap_{dataset}.pdf'
	resources:
		partition='short',
		time='4:00:00',
		mem_mb=168000,
		disk_mb=168000
	run:
		import scanpy as sc
		import scvelo as scv
		import matplotlib.pyplot as pl
		# project utils
		sys.path.insert(1, '../../')
		from utils import pseudo_bulk

		adata = sc.read(input[0])
		bdata = pseudo_bulk(adata, ['perturbation'])
		sc.pp.normalize_per_cell(bdata)
		sc.pp.log1p(bdata)
		sc.pp.pca(bdata)
		sc.pp.neighbors(bdata)
		sc.tl.umap(bdata)
		bdata.obs['perturbation'] = bdata.obs['perturbation'].astype('category')
		scv.pl.scatter(bdata, color='perturbation', show=False, dpi=120, legend_loc='right margin')
		pl.savefig(output[0], bbox_inches='tight', dpi=120)
		pl.close()

rule pseudobulk_correlations:
	"""
	Computes the pairwise correlation between pseudobulked perturbations in PCA space.
  	"""
	input: TEMPDIR / 'tmp_data_{dataset}.h5'
	output: 'figures/pseudobulk_correlations_{dataset}.pdf', 'tables/pseudobulk_correlations_{dataset}_tables.csv'
	resources:
		partition='short',
		time='4:00:00',
		mem_mb=168000,
		disk_mb=168000
	run:
		import scanpy as sc
		import matplotlib.pyplot as pl
		# project utils
		sys.path.insert(1, '../../')
		from utils import pseudo_bulk, cluster_matrix, plot_heatmap

		adata = sc.read(input[0])
		bdata = pseudo_bulk(adata, ['perturbation'])
		sc.pp.normalize_per_cell(bdata)
		sc.pp.log1p(bdata)
		sc.pp.pca(bdata)
		Z = bdata.obsm['X_pca']
		df = pd.DataFrame(Z, index=bdata.obs.perturbation)
		pca_corr = df.T.corr()
		tab = cluster_matrix(pca_corr, 'both')
		# plot
		plot_heatmap(tab, wildcards.dataset+' pseudobulk correlations')
		# export
		pl.savefig(output[0], bbox_inches='tight', dpi=120)
		tab.to_csv(output[1])
		pl.close()

rule effect_of_multi_vs_single_PCA:
	"""
	Computes the E-distance between control and each perturbations 
	1) using a PCA computed on all perturbations jointly
	2) re-computing PCA for each control-perturbation combi.
	Reasoning: PCA might be biased to represent highly abundant perturbations primarily.
	Multi-PCA is probably computationally more expensive.
	TODO: Run E-test each time.
  	"""
	input: TEMPDIR / 'tmp_data_{dataset}.h5', 'tables/etest_{dataset}_tables.csv'
	output: 
		'figures/multi_vs_sineffect_of_multi_vs_single_PCA_{dataset}.pdf', 
		'tables/effect_of_multi_vs_single_PCA_{dataset}_tables.csv'
	resources:
		partition='short',
		time='4:00:00',
		mem_mb=32000,
		disk_mb=32000
	run:
		import scanpy as sc
		import scvelo as scv
		import seaborn as sns
		import matplotlib.pyplot as pl
		from tqdm import tqdm
		# scperturb utils
		sys.path.insert(1, '../../package/src/')
		from scperturb import edist
		# project utils
		sys.path.insert(1, '../../')
		from utils import pseudo_bulk

		adata = sc.read(input[0])
		et = pd.read_csv(input[1], index_col=0)
		control = 'control'
		groupby = 'perturbation'

		res = {}
		for group in tqdm(adata.obs[groupby].unique()):
			if group==control:
				continue
			sdata = adata[adata.obs[groupby].isin([control, group])].copy()
			sc.pp.highly_variable_genes(sdata, n_top_genes=2000, layer='counts', flavor='seurat_v3')
			sc.pp.pca(sdata)
			res[group] = edist(sdata, 'perturbation', 'X_pca', verbose=False).iloc[0,1]

		ed_all = edist(adata)
		ed_all_control = ed_all.loc[:, control]
		ed_exclusive = pd.Series(res)
		df = pd.concat([ed_all_control, ed_exclusive], axis=1)
		df.columns = ['singlePCA', 'multiPCA']
		df = df.fillna(0)
		df = pd.concat([df, et], axis=1)
		df.to_csv(output[1])

		# scatter plot comparing E-distances
		with sns.axes_style('whitegrid'):
			fig, ax = pl.subplots(figsize=[7,5], dpi=100)
		sns.regplot(data=df, x='singlePCA', y='multiPCA', ax=ax, scatter_kws={'s': 0})
		sns.scatterplot(data=df, x='singlePCA', y='multiPCA', hue='significant_adj', ax=ax)
		from scipy.stats import spearmanr
		r = spearmanr(df.singlePCA, df.multiPCA)[0]
		ax.set_title(f'E-distances of to unperturbed in {wildcards.dataset}\nSpearman correlation: {np.round(r,2)}')
		ax.set_xlabel('with one joint PCA for all perturbations\n(singlePCA)')
		ax.set_ylabel('with one extra PCA for each perturbation\n(multiPCA)')
		pl.savefig(output[0], bbox_inches='tight', dpi=120)
		pl.close()

rule effect_of_nHVGs:
	"""
	Computes the E-distance for each perturbation to control at different number of HVGs for PCA.
  	"""
	input: TEMPDIR / 'tmp_data_{dataset}.h5'
	output: 'tables/effect_of_nHVGs{dataset}_tables.csv'
	resources:
		partition='short',
		time='4:00:00',
		mem_mb=64000,
		disk_mb=64000
	params:
		iterations=10
	run:
		import scanpy as sc
		import scvelo as scv
		import matplotlib.pyplot as pl
		# scperturb utils
		sys.path.insert(1, '../../package/src/')
		from scperturb import edist, onesided_pca_distances, etest, self_pca_distances

		adata = sc.read(input[0])
		control = 'control'
		groupby = 'perturbation'

		sigs = {}
		deltas = {}
		tests = {}
		nHVGs = {}

		# Define sample points ("nodes")
		# ns = np.linspace(1, 100, num=params.iterations)
		ns_ = leftsided_chebyshev_nodes(params.iterations)
		ns_ = np.array(ns_ * 100, dtype=int)
		ns_ = np.unique(ns_)
		ns = ns_[ns_>0]
		n_max = np.max(ns)

   		sc.pp.pca(adata, use_highly_variable=False, n_comps=n_max)
		for n in ns:
			nHVGs[n] = n
			tdata = adata.copy()
			tdata.obsm['X_pca'] = adata.obsm['X_pca'][:, :int(n)].copy()  # subset PCA to use only n first PCs
        	
			# STATS
			deltas[n] = onesided_pca_distances(tdata, obs_key=groupby, control=control, verbose=False)
			sigs[n] = self_pca_distances(tdata, groupby, verbose=False)
			# E-test
			tests[n] = etest(tdata, groupby, control='control', runs=100)

		sdf = pd.concat([
			pd.concat(tests),
			pd.concat(deltas),
			pd.concat(sigs),
			pd.concat(nHVGs)
		], axis=1).reset_index()
		sdf.columns = ['n', groupby, 'edist', 'pvalue', 'significant', 'pvalue_adj', 'significant_adj', 'delta', 'sigma', 'nHVGs']
		sdf['significant_adj'][sdf[groupby]==control]='control'
		# annotate newly insignificant
		sdf['significant_adj_new'] = sdf['significant_adj']
		for g in sdf.perturbation.unique():
			if sdf['significant_adj'][(sdf[groupby]==g) & (sdf['n']==1)].iloc[0]==True:
				sdf['significant_adj_new'][(sdf[groupby]==g) & (sdf['significant_adj']==False)] = 'False (lost)'
			elif sdf['significant_adj'][(sdf[groupby]==g) & (sdf['n']==1)].iloc[0]==False:
				sdf['significant_adj_new'][(sdf.perturbation==g) & (sdf['significant_adj']==True)] = 'True (gained)'
		sdf.to_csv(output[0])

rule effect_of_ncells:
	"""
	Subsamples number of cells, computes E-distance and E-test at each subsampling point.
	TODO: Currently subsamples the whole adata, make it subsample per perturbation independently!
	"""
	input: TEMPDIR / 'tmp_data_{dataset}.h5'
	output: 'tables/effect_of_ncells_{dataset}_tables.csv'
	resources:
		partition='medium',
		time='24:00:00',
		mem_mb=64000,
		disk_mb=64000
	params:
		iterations=10
	run:
		import scanpy as sc
		import scvelo as scv
		import matplotlib.pyplot as pl
		# scperturb utils
		sys.path.insert(1, '../../package/src/')
		from scperturb import edist, onesided_pca_distances, etest, self_pca_distances

		adata = sc.read(input[0])
		control = 'control'
		groupby = 'perturbation'

		sigs = {}
		deltas = {}
		tests = {}
		ncells = {}

		# Define sample points ("nodes")
		ns = leftsided_chebyshev_nodes(params.iterations)

		total_counts = np.sum(adata.X>0)
		for n in ns:
			# subsample cells
			print(n*tdata.n_obs)
			tdata = sc.pp.subsample(adata, fraction=n, copy=True)
			tdata.obs['ncells'] = tdata.X.sum(1)
			ncells[n] = tdata.obs[groupby].value_counts()
			sc.pp.pca(tdata, use_highly_variable=False)
			# STATS
			deltas[n] = onesided_pca_distances(tdata, obs_key=groupby, control=control, verbose=False)  # THIS THROWS AN ERROR
			# Found array with 0 sample(s) (shape=(0, 10)) while a minimum of 1 is required by check_pairwise_arrays.
			sigs[n] = self_pca_distances(tdata, groupby, verbose=False)
			# E-test
			tests[n] = etest(tdata, groupby, control='control', runs=100)

		sdf = pd.concat([
			pd.concat(tests),
			pd.concat(deltas),
			pd.concat(sigs),
			pd.concat(ncells)
		], axis=1).reset_index()
		sdf.columns = ['n', groupby, 'edist', 'pvalue', 'significant', 'pvalue_adj', 'significant_adj', 'delta', 'sigma', 'ncells']
		sdf['significant_adj'][sdf[groupby]==control]='control'
		# annotate newly insignificant
		sdf['significant_adj_new'] = sdf['significant_adj']
		for g in sdf.perturbation.unique():
			if sdf['significant_adj'][(sdf[groupby]==g) & (sdf['n']==1)].iloc[0]==True:
				sdf['significant_adj_new'][(sdf[groupby]==g) & (sdf['significant_adj']==False)] = 'False (lost)'
			elif sdf['significant_adj'][(sdf[groupby]==g) & (sdf['n']==1)].iloc[0]==False:
				sdf['significant_adj_new'][(sdf.perturbation==g) & (sdf['significant_adj']==True)] = 'True (gained)'
		sdf.to_csv(output[0])

rule effect_of_ncounts:
	"""
	Subsamples UMI counts, computes E-distance and E-test at each subsampling point per perturbation to control.
	"""
	input: TEMPDIR / 'tmp_data_{dataset}.h5'
	output: 'tables/effect_of_ncounts_{dataset}_tables.csv'
	resources:
		partition='medium',
		time='24:00:00',
		mem_mb=64000,
		disk_mb=64000
	params:
		iterations=10
	run:
		import scanpy as sc
		import scvelo as scv
		import matplotlib.pyplot as pl
		# scperturb utils
		sys.path.insert(1, '../../package/src/')
		from scperturb import edist, onesided_pca_distances, etest, self_pca_distances

		adata = sc.read(input[0])
		adata.X = adata.layers['counts'].copy()  # reset to raw counts
		control = 'control'
		groupby = 'perturbation'

		sigs = {}
		deltas = {}
		tests = {}
		ncounts = {}

		# Define sample points ("nodes")
		# ns = np.linspace(0.01, 1, num=params.iterations)
		ns = leftsided_chebyshev_nodes(params.iterations)

		total_counts = np.sum(adata.X>0)
		for n in ns:
			tdata = sc.pp.downsample_counts(adata, total_counts=int(n*total_counts), copy=True)
			tdata.obs['ncounts'] = tdata.X.sum(1)
			ncounts[n] = tdata.obs.groupby(groupby).mean()['ncounts']
			sc.pp.normalize_per_cell(tdata)
			sc.pp.log1p(tdata)
			sc.pp.highly_variable_genes(tdata, n_top_genes=2000)
			sc.pp.pca(tdata, use_highly_variable=False)
			# STATS
			deltas[n] = onesided_pca_distances(tdata, obs_key=groupby, control=control, verbose=False)
			sigs[n] = self_pca_distances(tdata, groupby, verbose=False)
			# E-test
			tests[n] = etest(tdata, groupby, control='control', runs=100)

		sdf = pd.concat([
			pd.concat(tests),
			pd.concat(deltas),
			pd.concat(sigs),
			pd.concat(ncounts)
		], axis=1).reset_index()
		sdf.columns = ['n', groupby, 'edist', 'pvalue', 'significant', 'pvalue_adj', 'significant_adj', 'delta', 'sigma', 'ncounts']
		sdf['significant_adj'][sdf[groupby]==control]='control'
		# annotate newly insignificant
		sdf['significant_adj_new'] = sdf['significant_adj']
		for g in sdf.perturbation.unique():
			if sdf['significant_adj'][(sdf[groupby]==g) & (sdf['n']==1)].iloc[0]==True:
				sdf['significant_adj_new'][(sdf[groupby]==g) & (sdf['significant_adj']==False)] = 'False (lost)'
			elif sdf['significant_adj'][(sdf[groupby]==g) & (sdf['n']==1)].iloc[0]==False:
				sdf['significant_adj_new'][(sdf.perturbation==g) & (sdf['significant_adj']==True)] = 'True (gained)'
		sdf.to_csv(output[0])
